{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝 개념과 활용\n",
    "\n",
    "## 머신 러닝 : 컴퓨터 프로그램이다. \n",
    "어떠한 작업을 몇개의 클래스로 나눠서 그 클래스들을 작업하는 여러가지 경험을 통하여\n",
    "컴퓨터가 배울 수 있는 프로그램이라고 정의하였다.\n",
    "컴퓨터가 데이터들을 활용해서 컴퓨터 스스로가 문제를 해결하는 방법을 찾아나가 퍼포먼스를 올릴 수 있는 프로그램\n",
    "ex) 로지스틱 회귀분석, 기업 평가, 기상청 정보 등..\n",
    "\n",
    "## 딥 러닝 : 머신 러닝의 한 분야\n",
    "학습하는 방식에서 차이가 난다.\n",
    "기계들은 일반적으로 로우 레벨에서 하이 레벨로 증가시키면서 학습을 한다.\n",
    "예를 들어서, 빛과 색 정보를 가진 사진 픽셀에서 특징들을 표상해내고, 마지막으로 조건, 특징에 맞게 분류해내고 구분한다.\n",
    "\n",
    "## 머신러닝과 딥 러닝의 비교\n",
    "\n",
    "### 2.1 데이터 의존도\n",
    "- 딥러닝과 전통적인 머신러닝의 가장 큰 차이점은 데이터 양에 따른 성능이다.\n",
    "- 딥러닝 알고리즘은 어떤 과제를 이해하기 위해서 매우 큰 데이터가 필요하기 때문에 데이터가 작을때는 성능이 잘 나오지 않으나\n",
    "  데이터의 양이 커지면 성능을 계속 향상시킬 수 있음\n",
    "  \n",
    "### 2.2 하드웨어 의존도\n",
    "- 딥러닝 알고리즘은 고사양 머신이 많은 부분을 좌지우지 됨\n",
    "- 머신러닝은 저사양 머신에서도 실행이 가능\n",
    "- 딥러닝 알고리즘의 요구사항은 GPU가 포함되기 때문이고, GPU는 작업에서 숫자계산을 담당함.\n",
    "- 딥러닝은 많은 양의 행렬곱셈을 수행해야 하기 때문에 GPU를 사용하면 보다 효율적으로 최적화 할 수 있음.\n",
    "\n",
    "### 2.3 Feature engineering\n",
    "- Feature engineering은 데이터 복잡성을 줄이고 학습 알고리즘에서 패턴을 보다 잘 보이게 하는 과정\n",
    "- 이 과정은 많은 시간과 전문가가 필요하다는 점에서 어렵고 비쌈.\n",
    "- 머신러닝은 대부분의 적용된 변수(feature)는 전문가가 식별한 다음 정보 영역 및 데이터 유형별로 코딩함.\n",
    "  예) 신용평가 모형 개발에서 사용되는 50여개의 변수 중 비즈니스 여건이나 금융당국에 의해 제거되어야 하는\n",
    "      변수(학력, 성별 등)가 있을 수 있기 때문에 전문가가 직접 확인하고 걸러내는 작업이 필요\n",
    "- 딥러닝은 high-level features를 학습하며 모든 과제에서 새로운 변수 추출이라는 작업을 줄여줌\n",
    "- Convolutional Neural Network의 경우, 초기 layer에서는 이미지의 edge나 line같은 low-level features를 학습하고\n",
    "  그 다음 이미지의 high-level 표현을 학습함.\n",
    "\n",
    "### 2.4 문제 해결 접근법\n",
    "- 전통적인 머신러닝 알고리즘으로 문제를 해결할 때는 주로 문제를 여러 개의 파트로 나눈 후, 각각에 대한 답을 구하고\n",
    "  그 결과를 합치는 방법을 추천\n",
    "- 딥러닝은 end-to-end 방식으로 문제를 해결\n",
    "\n",
    "#### 사물인지  프로젝트의 경우,\n",
    "- 머신러닝 접근 방법은 1. 사물탐지 / 2. 사물인지 2단계로 나누어 학습 함.\n",
    "1. 사물탐색 : grabcout과 같은 경계탐지 알고리즘(bound box detection algorithm)을 사용하여 이미지를\n",
    "              훑어보고 가능한 모든 객체를 추출\n",
    "2. 사물인지 : 모든 객체들을 SVM과 같은 객체 인식 알고리즘으로 관련 객체를 인식함\n",
    "- 딥러닝 접근 방법은 end-to-end 방식인 YOLO net에서 이미지를 전달하면 객체의 이름과 위치가 표시 됨.\n",
    "\n",
    "### 2.5 실행 시간\n",
    "- 딥 러닝 알고리즘은 훈련 시간이 굉장히 오래 걸림\n",
    "- 딥 러닝 알고르짐은 다른 알고리즘에 비해 변수가 너무 많기 때문에 ResNet의 경우, training이 약 2주 정도 걸림\n",
    "- 머신 러닝의 경우, 수초에서 수시간이면 거의 훈련이 끝남.\n",
    "\n",
    "### 2.6 해석력\n",
    "- 이 요인이 딥 러닝을 실제 실무에 쓰려 마음 먹으면 그 전에 10번 정도 고민하는 이유\n",
    "- 에세이의 점수를 자동으로 매기는 과제의 경우, 딥 러닝의 성능은 사람이 한 것과 유사할 정도로 뛰어나지만 아주 치명적인 문제가 있음\n",
    "  -> 왜 이 점수가 부여됐는지 알 수가 없기 때문에\n",
    "- 수학적으로 딥 러닝의 어느 노드가 활성화 되었는지는 알 수 있지만 우리는 거기에 어떤 뉴련이 만들어 졌는지,\n",
    "  그리고 이들 뉴련 레이어가 전체적으로 무엇을 하고 있는지 알지 못하기 때문에 해석하기 힘듦\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인공신경망(Artificial Neural Network)\n",
    "- 인간 뇌를 기반으로 한 추론 모델.\n",
    "- 인간 뇌의 추론 모델-뉴런(neuron)\n",
    "- 뉴런은 기본적인 정보처리 단위\n",
    "\n",
    "## 인간 뇌의 특징\n",
    "- 100억개의 뉴런과 각 뉴런을 연결하는 6조 개의 시냅스의 결합체\n",
    "- 인간의 뇌는 현존하는 어떤 컴퓨터보다 빠르게 기능을 수행할 수 있음\n",
    "- 인간의 뇌는 매우 복잡하고, 비선형적이며, 병렬적인 정보 처리 시스템으로 생각할 수 있음\n",
    "- 정보는 신경망 전체에 동시에 저장되고 처리됨\n",
    "- 적응성에 따라 '잘못된 답'으로 이끄는 뉴런들 사이의 연결은 약화되고,\n",
    "  '올바른 답'으로 이끄는 연결은 강화됨\n",
    "  \n",
    "## 인간의 뇌 모델링\n",
    "- 생물학적인 뇌의 뉴런과 비슷하게 모델링 함\n",
    "- 인공신경망은 뉴런이라는 아주 단순하지만 내부적으로 매우 복잡하게 연결된 프로세스들로 이루어져 있음\n",
    "- 뉴런은 가중치가 있는 링크들로 연결되어 있음\n",
    "- 각각의 뉴런은 연결을 통해 여러 입력 신호를 받지만 출력 신호는 오직 하나만 생성\n",
    "- 인공 신경망 구조\n",
    "- input layer -> hidden layer1 -> hidden layer2 -> output layer\n",
    "\n",
    "\n",
    "## 인공 신경망의 학습\n",
    "- 신경망은 가중치를 반복적으로 조정하여 학습\n",
    "- 뉴런은 링크(link)로 연결되어 있고, 각 링크에는 그와 연관된 수치적인 가중치가 있음\n",
    "- 가중치는 장기 기억을 위한 기본적인 수단으로 각 뉴런 입력 강도, 즉 중요도를 표현\n",
    "\n",
    "## 인공 신경망의 가중치 조정\n",
    "- 신경망의 가중치를 초기화하고 훈련 예제들의 집합에서 해당 가중치를 갱신\n",
    "- 신경망의 구조를 먼저 선택하고, 어떤 학습 알고리즘을 사용할 지 결정한 후 신경망을 훈련시킴\n",
    "\n",
    "## 뉴런의 특징\n",
    "- 입력 링크에서 여러 신호를 받아서 새로운 활성화 수준을 계산하고, 출력 링크로 출력 신호를 보냄\n",
    "- 입력신호는 미가공 데이터 또는 다른 뉴런의 출력이 될 수 있음\n",
    "- 출력신호는 문제의 최종적인 해(solution)가 되거나 다른 뉴런에 입력 될 수 있음\n",
    "- x0(뉴런에서 나온 축색돌기) -> w0(시넵스) -> w0x0(가지돌기) --> sum(wixi)+b (뉴런) --> f(활성화함수) --> f(sum(wixi)+b) (출력용 축색돌기)\n",
    "\n",
    "## 뉴런의 계산\n",
    "- 뉴런은 전이함수, 즉 활성화 함수(activation function)을 사용\n",
    "- 활성화 함수를 이용한 출력 결정 순서\n",
    "    1. 뉴런은 입력 신호의 가중치 합을 계산하여 임계값과 비교\n",
    "    2. 가중치 합이 임계값보다 작으면 뉴런의 출력은 -1, 같거나 크면 +1을 출력함\n",
    "- 활성화 함수 ( X : 뉴런으로 돌아가는 입력의 순 가중합, xi : 입력 i의 값, wi : 입력 i의 가중치\n",
    "                n : 뉴런의 입력 개수 , Y는 뉴런의 출력\n",
    "= X = SUM(xiwi)(i=1~n)\n",
    "= Y = {+1  if X >= θ\n",
    "      {-1  if X < θ\n",
    "      \n",
    "## 뉴런의 출력 결정\n",
    "- 활성화 함수를 부호함수를 사용하는 뉴런의 실제 출력\n",
    "- ex) 계단 함수, 부호 함수, 시그모이드 함수, 선형 함수\n",
    "\n",
    "## 단일 뉴런의 학습( 단층 퍼셉트론 )\n",
    "- 퍼셉트론은 선형 결합기와 하드 리미터로 구성\n",
    "- 초평면(hyperplane)으로 n차원 공간을 두 개의 결정 영역으로 나눔\n",
    "- 선형 분리 함수 : sum(i=1~n)(xiwi) - θ = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AND 연산자 학습(임계값 θ=0.2, 학습률 α=0.1)\n",
    "\n",
    "AND 연산자 : 입력값 두 가지가 모두 1일 때 출력값이 1이 나오는 경우\n",
    "OR  연산자 : 입력값 두 가지 중 하나라도 1이면 출력값이 1이 나오는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jsdata\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "## x_data : 데이터\n",
    "## y_data : 결과값\n",
    "\n",
    "x_data = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [1]], dtype=np.float32)\n",
    "\n",
    "# X, Y : placeholder로 지정해서 세션을 run시킬때 입력값으로 활용할 수 있도록 함\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# W = 입력값이 (x,y)이므로 [2,1]로 설정\n",
    "# b = 입력값이 하나만 필요하므로 [1]로 설정\n",
    "\n",
    "W = tf.Variable(tf.random_normal([2,1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis using sigmoid : tf.div(1., 1. + tf.exp(tf.matmul(X,W)))\n",
    "# X, W를 matrix에 multiply하고 -b를 적용해준다.\n",
    "hypothesis = tf.sigmoid(tf.matmul(X,W)-b)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0 \n",
      "cost : 0.5098366 \n",
      "Weight :\n",
      " [[0.22865255]\n",
      " [0.20973209]] \n",
      "bias :\n",
      " [-0.90808624]\n",
      "Hypothesis:\n",
      " [[0.71260834]\n",
      " [0.7535838 ]\n",
      " [0.7570804 ]\n",
      " [0.79355204]] \n",
      "Correct:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.75\n",
      "step : 500 \n",
      "cost : 0.16170466 \n",
      "Weight :\n",
      " [[2.762675 ]\n",
      " [2.7590458]] \n",
      "bias :\n",
      " [0.74690866]\n",
      "Hypothesis:\n",
      " [[0.3214953 ]\n",
      " [0.88206553]\n",
      " [0.8824426 ]\n",
      " [0.9916309 ]] \n",
      "Correct:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.75\n",
      "step : 1000 \n",
      "cost : 0.09174825 \n",
      "Weight :\n",
      " [[3.9722285]\n",
      " [3.9709647]] \n",
      "bias :\n",
      " [1.444523]\n",
      "Hypothesis:\n",
      " [[0.19084588]\n",
      " [0.92597485]\n",
      " [0.9260614 ]\n",
      " [0.9984969 ]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n",
      "step : 1500 \n",
      "cost : 0.063016415 \n",
      "Weight :\n",
      " [[4.7600226]\n",
      " [4.7594123]] \n",
      "bias :\n",
      " [1.8683934]\n",
      "Hypothesis:\n",
      " [[0.13372767]\n",
      " [0.9474007 ]\n",
      " [0.947431  ]\n",
      " [0.9995247 ]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n",
      "step : 2000 \n",
      "cost : 0.04766138 \n",
      "Weight :\n",
      " [[5.3391004]\n",
      " [5.33875  ]] \n",
      "bias :\n",
      " [2.1723628]\n",
      "Hypothesis:\n",
      " [[0.10225984]\n",
      " [0.95954955]\n",
      " [0.95956314]\n",
      " [0.99979764]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n",
      "step : 2500 \n",
      "cost : 0.038192842 \n",
      "Weight :\n",
      " [[5.7950273]\n",
      " [5.794795 ]] \n",
      "bias :\n",
      " [2.4087324]\n",
      "Hypothesis:\n",
      " [[0.08250922]\n",
      " [0.96726614]\n",
      " [0.9672735 ]\n",
      " [0.999897  ]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n",
      "step : 3000 \n",
      "cost : 0.031802006 \n",
      "Weight :\n",
      " [[6.1701493]\n",
      " [6.1699934]] \n",
      "bias :\n",
      " [2.6017742]\n",
      "Hypothesis:\n",
      " [[0.06902432]\n",
      " [0.97256774]\n",
      " [0.97257185]\n",
      " [0.999941  ]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n",
      "step : 3500 \n",
      "cost : 0.02721144 \n",
      "Weight :\n",
      " [[6.4883766]\n",
      " [6.4882627]] \n",
      "bias :\n",
      " [2.7647297]\n",
      "Hypothesis:\n",
      " [[0.0592601]\n",
      " [0.9764209]\n",
      " [0.9764235]\n",
      " [0.9999633]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n",
      "step : 4000 \n",
      "cost : 0.02376076 \n",
      "Weight :\n",
      " [[6.7644596]\n",
      " [6.764369 ]] \n",
      "bias :\n",
      " [2.9056084]\n",
      "Hypothesis:\n",
      " [[0.05187699]\n",
      " [0.9793416 ]\n",
      " [0.9793434 ]\n",
      " [0.9999757 ]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n",
      "step : 4500 \n",
      "cost : 0.021075577 \n",
      "Weight :\n",
      " [[7.0081096]\n",
      " [7.0080414]] \n",
      "bias :\n",
      " [3.029614]\n",
      "Hypothesis:\n",
      " [[0.04610583]\n",
      " [0.9816288 ]\n",
      " [0.9816301 ]\n",
      " [0.9999831 ]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n",
      "step : 5000 \n",
      "cost : 0.018928505 \n",
      "Weight :\n",
      " [[7.2260585]\n",
      " [7.226005 ]] \n",
      "bias :\n",
      " [3.140314]\n",
      "Hypothesis:\n",
      " [[0.04147458]\n",
      " [0.9834665 ]\n",
      " [0.98346734]\n",
      " [0.9999877 ]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n",
      "step : 5500 \n",
      "cost : 0.017173678 \n",
      "Weight :\n",
      " [[7.423152]\n",
      " [7.423105]] \n",
      "bias :\n",
      " [3.2402587]\n",
      "Hypothesis:\n",
      " [[0.03767851]\n",
      " [0.98497427]\n",
      " [0.98497486]\n",
      " [0.9999909 ]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n",
      "step : 6000 \n",
      "cost : 0.015713282 \n",
      "Weight :\n",
      " [[7.6029882]\n",
      " [7.6029487]] \n",
      "bias :\n",
      " [3.3313339]\n",
      "Hypothesis:\n",
      " [[0.03451174]\n",
      " [0.986233  ]\n",
      " [0.9862335 ]\n",
      " [0.999993  ]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n",
      "step : 6500 \n",
      "cost : 0.014479421 \n",
      "Weight :\n",
      " [[7.7683187]\n",
      " [7.768284 ]] \n",
      "bias :\n",
      " [3.4149714]\n",
      "Hypothesis:\n",
      " [[0.03183088]\n",
      " [0.9872992 ]\n",
      " [0.9872997 ]\n",
      " [0.99999464]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n",
      "step : 7000 \n",
      "cost : 0.013423437 \n",
      "Weight :\n",
      " [[7.921287]\n",
      " [7.921257]] \n",
      "bias :\n",
      " [3.4922833]\n",
      "Hypothesis:\n",
      " [[0.02953258]\n",
      " [0.98821384]\n",
      " [0.98821414]\n",
      " [0.9999957 ]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n",
      "step : 7500 \n",
      "cost : 0.012509712 \n",
      "Weight :\n",
      " [[8.063597]\n",
      " [8.063571]] \n",
      "bias :\n",
      " [3.5641515]\n",
      "Hypothesis:\n",
      " [[0.02754101]\n",
      " [0.9890067 ]\n",
      " [0.98900706]\n",
      " [0.99999654]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n",
      "step : 8000 \n",
      "cost : 0.0117114065 \n",
      "Weight :\n",
      " [[8.196626]\n",
      " [8.196602]] \n",
      "bias :\n",
      " [3.6312873]\n",
      "Hypothesis:\n",
      " [[0.02579883]\n",
      " [0.98970056]\n",
      " [0.9897009 ]\n",
      " [0.99999714]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n",
      "step : 8500 \n",
      "cost : 0.011008173 \n",
      "Weight :\n",
      " [[8.3215 ]\n",
      " [8.32148]] \n",
      "bias :\n",
      " [3.6942701]\n",
      "Hypothesis:\n",
      " [[0.02426228]\n",
      " [0.9903127 ]\n",
      " [0.99031293]\n",
      " [0.9999976 ]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n",
      "step : 9000 \n",
      "cost : 0.010383909 \n",
      "Weight :\n",
      " [[8.4391575]\n",
      " [8.439138 ]] \n",
      "bias :\n",
      " [3.7535803]\n",
      "Hypothesis:\n",
      " [[0.02289706]\n",
      " [0.99085677]\n",
      " [0.990857  ]\n",
      " [0.999998  ]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n",
      "step : 9500 \n",
      "cost : 0.009826167 \n",
      "Weight :\n",
      " [[8.550377]\n",
      " [8.550361]] \n",
      "bias :\n",
      " [3.8096213]\n",
      "Hypothesis:\n",
      " [[0.0216763]\n",
      " [0.9913435]\n",
      " [0.9913435]\n",
      " [0.9999984]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n",
      "step : 10000 \n",
      "cost : 0.009324882 \n",
      "Weight :\n",
      " [[8.65583 ]\n",
      " [8.655814]] \n",
      "bias :\n",
      " [3.8627305]\n",
      "Hypothesis:\n",
      " [[0.02057821]\n",
      " [0.9917813 ]\n",
      " [0.99178135]\n",
      " [0.9999986 ]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Accuracy computation\n",
    "# True if hypothesis > 0.2 else False\n",
    "# predicted(예측값)을 통하여 hypothesis가 0.2보다 크냐 작으냐에 따라 1,0으로 지정\n",
    "predicted = tf.cast(hypothesis > 0.2, dtype = tf.float32)\n",
    "# cast를 사용하여 예측값과 Y값이 같으면 1, 다르면 0으로 accuracy 함수 만들어줌\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 500 == 0:\n",
    "            print(\"step :\", step, \"\\ncost :\", sess.run(cost, feed_dict={X: x_data, Y: y_data}),\n",
    "                  \"\\nWeight :\\n\", sess.run(W), \"\\nbias :\\n\", sess.run(b))\n",
    "            h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict = {X: x_data, Y: y_data})\n",
    "            print(\"Hypothesis:\\n\", h, \"\\nCorrect:\\n\", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR using NN\n",
    "XOR의 속성\n",
    "- x1과 x2가 서로 다를 때만 그 결과가 1이고, 나머지 경우에는 결과값을 가짐\n",
    "- 그래프로 표현했을 때 결과값 1과 0에 대해서 하나의 직선으로 정확히 나눌 수가 없음\n",
    "\n",
    "## One logistic regression unit cannot separate XOR\n",
    "XOR 문제는 단순히 하나의 모델로는 풀이가 불가능하다.\n",
    "->> 여러 개를 합치게 되면 가능해진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0 \n",
      "cost : 0.82376504 \n",
      "Weight :\n",
      " [[ 0.49596712]\n",
      " [-0.5773657 ]] \n",
      "bias :\n",
      " [0.9428371]\n",
      "Hypothesis:\n",
      " [[0.28032762]\n",
      " [0.17943165]\n",
      " [0.39010522]\n",
      " [0.2642032 ]] \n",
      "Correct:\n",
      " [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.25\n",
      "step : 500 \n",
      "cost : 0.6933083 \n",
      "Weight :\n",
      " [[0.06732345]\n",
      " [0.01908669]] \n",
      "bias :\n",
      " [0.05124907]\n",
      "Hypothesis:\n",
      " [[0.48719054]\n",
      " [0.4919601 ]\n",
      " [0.5040185 ]\n",
      " [0.50878936]] \n",
      "Correct:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.5\n",
      "step : 1000 \n",
      "cost : 0.6931497 \n",
      "Weight :\n",
      " [[0.00710258]\n",
      " [0.00500377]] \n",
      "bias :\n",
      " [0.00717991]\n",
      "Hypothesis:\n",
      " [[0.49820504]\n",
      " [0.49945596]\n",
      " [0.49998066]\n",
      " [0.5012316 ]] \n",
      "Correct:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.5\n",
      "step : 1500 \n",
      "cost : 0.6931472 \n",
      "Weight :\n",
      " [[0.00089368]\n",
      " [0.00080233]] \n",
      "bias :\n",
      " [0.00100585]\n",
      "Hypothesis:\n",
      " [[0.49974853]\n",
      " [0.49994913]\n",
      " [0.49997196]\n",
      " [0.50017256]] \n",
      "Correct:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.5\n",
      "step : 2000 \n",
      "cost : 0.6931472 \n",
      "Weight :\n",
      " [[0.00012078]\n",
      " [0.00011681]] \n",
      "bias :\n",
      " [0.0001409]\n",
      "Hypothesis:\n",
      " [[0.49996477]\n",
      " [0.49999398]\n",
      " [0.49999496]\n",
      " [0.5000242 ]] \n",
      "Correct:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.5\n",
      "step : 2500 \n",
      "cost : 0.6931472 \n",
      "Weight :\n",
      " [[1.6719061e-05]\n",
      " [1.6548802e-05]] \n",
      "bias :\n",
      " [1.9718245e-05]\n",
      "Hypothesis:\n",
      " [[0.49999508]\n",
      " [0.4999992 ]\n",
      " [0.49999925]\n",
      " [0.5000034 ]] \n",
      "Correct:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.5\n",
      "step : 3000 \n",
      "cost : 0.6931472 \n",
      "Weight :\n",
      " [[2.3260252e-06]\n",
      " [2.3196828e-06]] \n",
      "bias :\n",
      " [2.7577266e-06]\n",
      "Hypothesis:\n",
      " [[0.4999993 ]\n",
      " [0.49999988]\n",
      " [0.49999988]\n",
      " [0.5000005 ]] \n",
      "Correct:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.5\n",
      "step : 3500 \n",
      "cost : 0.6931472 \n",
      "Weight :\n",
      " [[3.2330257e-07]\n",
      " [3.2292104e-07]] \n",
      "bias :\n",
      " [4.3239982e-07]\n",
      "Hypothesis:\n",
      " [[0.49999988]\n",
      " [0.49999997]\n",
      " [0.49999997]\n",
      " [0.50000006]] \n",
      "Correct:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.5\n",
      "step : 4000 \n",
      "cost : 0.6931472 \n",
      "Weight :\n",
      " [[8.9353598e-08]\n",
      " [8.8972065e-08]] \n",
      "bias :\n",
      " [5.9124307e-08]\n",
      "Hypothesis:\n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Correct:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.5\n",
      "step : 4500 \n",
      "cost : 0.6931472 \n",
      "Weight :\n",
      " [[8.9353598e-08]\n",
      " [8.8972065e-08]] \n",
      "bias :\n",
      " [5.9124307e-08]\n",
      "Hypothesis:\n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Correct:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.5\n",
      "step : 5000 \n",
      "cost : 0.6931472 \n",
      "Weight :\n",
      " [[8.9353598e-08]\n",
      " [8.8972065e-08]] \n",
      "bias :\n",
      " [5.9124307e-08]\n",
      "Hypothesis:\n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Correct:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.5\n",
      "step : 5500 \n",
      "cost : 0.6931472 \n",
      "Weight :\n",
      " [[8.9353598e-08]\n",
      " [8.8972065e-08]] \n",
      "bias :\n",
      " [5.9124307e-08]\n",
      "Hypothesis:\n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Correct:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.5\n",
      "step : 6000 \n",
      "cost : 0.6931472 \n",
      "Weight :\n",
      " [[8.9353598e-08]\n",
      " [8.8972065e-08]] \n",
      "bias :\n",
      " [5.9124307e-08]\n",
      "Hypothesis:\n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Correct:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.5\n",
      "step : 6500 \n",
      "cost : 0.6931472 \n",
      "Weight :\n",
      " [[8.9353598e-08]\n",
      " [8.8972065e-08]] \n",
      "bias :\n",
      " [5.9124307e-08]\n",
      "Hypothesis:\n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Correct:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.5\n",
      "step : 7000 \n",
      "cost : 0.6931472 \n",
      "Weight :\n",
      " [[8.9353598e-08]\n",
      " [8.8972065e-08]] \n",
      "bias :\n",
      " [5.9124307e-08]\n",
      "Hypothesis:\n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Correct:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.5\n",
      "step : 7500 \n",
      "cost : 0.6931472 \n",
      "Weight :\n",
      " [[8.9353598e-08]\n",
      " [8.8972065e-08]] \n",
      "bias :\n",
      " [5.9124307e-08]\n",
      "Hypothesis:\n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Correct:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.5\n",
      "step : 8000 \n",
      "cost : 0.6931472 \n",
      "Weight :\n",
      " [[8.9353598e-08]\n",
      " [8.8972065e-08]] \n",
      "bias :\n",
      " [5.9124307e-08]\n",
      "Hypothesis:\n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Correct:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.5\n",
      "step : 8500 \n",
      "cost : 0.6931472 \n",
      "Weight :\n",
      " [[8.9353598e-08]\n",
      " [8.8972065e-08]] \n",
      "bias :\n",
      " [5.9124307e-08]\n",
      "Hypothesis:\n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Correct:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.5\n",
      "step : 9000 \n",
      "cost : 0.6931472 \n",
      "Weight :\n",
      " [[8.9353598e-08]\n",
      " [8.8972065e-08]] \n",
      "bias :\n",
      " [5.9124307e-08]\n",
      "Hypothesis:\n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Correct:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.5\n",
      "step : 9500 \n",
      "cost : 0.6931472 \n",
      "Weight :\n",
      " [[8.9353598e-08]\n",
      " [8.8972065e-08]] \n",
      "bias :\n",
      " [5.9124307e-08]\n",
      "Hypothesis:\n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Correct:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.5\n",
      "step : 10000 \n",
      "cost : 0.6931472 \n",
      "Weight :\n",
      " [[8.9353598e-08]\n",
      " [8.8972065e-08]] \n",
      "bias :\n",
      " [5.9124307e-08]\n",
      "Hypothesis:\n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Correct:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "x_data = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "W = tf.Variable(tf.random_normal([2,1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis using sigmoid : tf.div(1., 1. + tf.exp(tf.matmul(X,W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X,W)-b)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis > 0.2 else False\n",
    "predicted = tf.cast(hypothesis > 0.2, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 500 == 0:\n",
    "            print(\"step :\", step, \"\\ncost :\", sess.run(cost, feed_dict={X: x_data, Y: y_data}),\n",
    "                  \"\\nWeight :\\n\", sess.run(W), \"\\nbias :\\n\", sess.run(b))\n",
    "            h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict = {X: x_data, Y: y_data})\n",
    "            print(\"Hypothesis:\\n\", h, \"\\nCorrect:\\n\", c, \"\\nAccuracy: \", a)\n",
    "            \n",
    "            \n",
    "## 즉, one logistic regression 에서는 하나의 퍼셉트론으로 문제를 해결 못함을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0 \n",
      "cost : 0.92540264 \n",
      "Weight :\n",
      " [array([[-1.4460417 ,  0.35130438],\n",
      "       [ 0.18383317,  2.5311675 ]], dtype=float32), array([[-0.49194542],\n",
      "       [-0.6575601 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 1.320458 , -0.1155914], dtype=float32), array([0.9304179], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.39313802 0.8072944 ]\n",
      " [0.21387097 0.883084  ]\n",
      " [0.39876488 0.8319189 ]\n",
      " [0.23772341 0.8909129 ]] \n",
      "Correct:\n",
      " [[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]] \n",
      "Accuracy:  0.5\n",
      "step : 500 \n",
      "cost : 0.6914418 \n",
      "Weight :\n",
      " [array([[-1.3426148 , -0.07969845],\n",
      "       [ 0.6375534 ,  2.3076763 ]], dtype=float32), array([[-0.49194542],\n",
      "       [-0.6575601 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 1.5264686 , -0.41097993], dtype=float32), array([0.56759703], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.5517846  0.45236525]\n",
      " [0.45161116 0.47272378]\n",
      " [0.5574944  0.52586967]\n",
      " [0.45484918 0.52717537]] \n",
      "Correct:\n",
      " [[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]] \n",
      "Accuracy:  0.5\n",
      "step : 1000 \n",
      "cost : 0.6831541 \n",
      "Weight :\n",
      " [array([[-1.47994   , -0.19579437],\n",
      "       [ 1.1144131 ,  2.274905  ]], dtype=float32), array([[-0.49194542],\n",
      "       [-0.6575601 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 1.6062646 , -0.31392613], dtype=float32), array([0.6535556], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.5313722  0.45394894]\n",
      " [0.4480935  0.4668895 ]\n",
      " [0.56122315 0.53998995]\n",
      " [0.46379566 0.51361084]] \n",
      "Correct:\n",
      " [[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]] \n",
      "Accuracy:  0.5\n",
      "step : 1500 \n",
      "cost : 0.66596925 \n",
      "Weight :\n",
      " [array([[-1.8433807 , -0.24019122],\n",
      "       [ 1.7304678 ,  2.2545683 ]], dtype=float32), array([[-0.49194542],\n",
      "       [-0.6575601 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 1.6965299, -0.2560187], dtype=float32), array([0.8277344], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.5099681  0.44350007]\n",
      " [0.4366688  0.4514402 ]\n",
      " [0.5817874  0.5787263 ]\n",
      " [0.4635548  0.49998727]] \n",
      "Correct:\n",
      " [[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]] \n",
      "Accuracy:  0.5\n",
      "step : 2000 \n",
      "cost : 0.63033485 \n",
      "Weight :\n",
      " [array([[-2.4266307 , -0.20924242],\n",
      "       [ 2.5373077 ,  2.2411525 ]], dtype=float32), array([[-0.49194542],\n",
      "       [-0.6575601 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 1.8677663 , -0.26272297], dtype=float32), array([1.0738052], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.47665325 0.4142827 ]\n",
      " [0.4186869  0.43054652]\n",
      " [0.6332831  0.649668  ]\n",
      " [0.45353502 0.48029906]] \n",
      "Correct:\n",
      " [[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]] \n",
      "Accuracy:  0.5\n",
      "step : 2500 \n",
      "cost : 0.57955617 \n",
      "Weight :\n",
      " [array([[-3.0984488, -0.1360212],\n",
      "       [ 3.4197857,  2.2707584]], dtype=float32), array([[-0.49194542],\n",
      "       [-0.6575601 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 2.1506457 , -0.36084613], dtype=float32), array([1.344356], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.42592406 0.36590797]\n",
      " [0.40524006 0.41769028]\n",
      " [0.7092838  0.7336909 ]\n",
      " [0.4394043  0.46080697]] \n",
      "Correct:\n",
      " [[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]] \n",
      "Accuracy:  0.5\n",
      "step : 3000 \n",
      "cost : 0.5292077 \n",
      "Weight :\n",
      " [array([[-3.7152684 , -0.17518386],\n",
      "       [ 4.214401  ,  2.3855126 ]], dtype=float32), array([[-0.49194542],\n",
      "       [-0.6575601 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 2.4580293, -0.5665126], dtype=float32), array([1.5955632], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.36500412 0.3091308 ]\n",
      " [0.40618467 0.42112535]\n",
      " [0.77861315 0.7965947 ]\n",
      " [0.43288663 0.450695  ]] \n",
      "Correct:\n",
      " [[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]] \n",
      "Accuracy:  0.5\n",
      "step : 3500 \n",
      "cost : 0.46174625 \n",
      "Weight :\n",
      " [array([[-4.258034 , -0.924392 ],\n",
      "       [ 4.8749266,  2.4095905]], dtype=float32), array([[-0.49194542],\n",
      "       [-0.6575601 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 2.6981468, -1.0862428], dtype=float32), array([1.9172739], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.3136757  0.2648954 ]\n",
      " [0.4409048  0.4626042 ]\n",
      " [0.81997955 0.82037604]\n",
      " [0.40330988 0.39781278]] \n",
      "Correct:\n",
      " [[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]] \n",
      "Accuracy:  0.5\n",
      "step : 4000 \n",
      "cost : 0.30598268 \n",
      "Weight :\n",
      " [array([[-4.7611127, -2.6544025],\n",
      "       [ 5.3361816,  2.895976 ]], dtype=float32), array([[-0.49194542],\n",
      "       [-0.6575601 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 2.8885365, -1.6367838], dtype=float32), array([2.2420704], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.2703419  0.23949492]\n",
      " [0.6032531  0.6464132 ]\n",
      " [0.85350674 0.84992224]\n",
      " [0.2717417  0.24350801]] \n",
      "Correct:\n",
      " [[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]] \n",
      "Accuracy:  0.5\n",
      "step : 4500 \n",
      "cost : 0.19555844 \n",
      "Weight :\n",
      " [array([[-5.19499  , -3.7499547],\n",
      "       [ 5.6574717,  3.6727467]], dtype=float32), array([[-0.49194542],\n",
      "       [-0.6575601 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 3.0012662, -1.9620194], dtype=float32), array([2.3718832], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.19213563 0.1714069 ]\n",
      " [0.753136   0.7833833 ]\n",
      " [0.8736806  0.8715732 ]\n",
      " [0.17602357 0.15576851]] \n",
      "Correct:\n",
      " [[0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]] \n",
      "Accuracy:  1.0\n",
      "step : 5000 \n",
      "cost : 0.13635173 \n",
      "Weight :\n",
      " [array([[-5.5232673, -4.4091697],\n",
      "       [ 5.903535 ,  4.225087 ]], dtype=float32), array([[-0.49194542],\n",
      "       [-0.6575601 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 3.0792081, -2.2325606], dtype=float32), array([2.5265522], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.13827053 0.1251438 ]\n",
      " [0.8366263  0.8548867 ]\n",
      " [0.8938472  0.89271605]\n",
      " [0.1226106  0.11008444]] \n",
      "Correct:\n",
      " [[0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]] \n",
      "Accuracy:  1.0\n",
      "step : 5500 \n",
      "cost : 0.10284962 \n",
      "Weight :\n",
      " [array([[-5.769067 , -4.8448615],\n",
      "       [ 6.1001453,  4.615014 ]], dtype=float32), array([[-0.49194542],\n",
      "       [-0.6575601 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 3.144931 , -2.4355283], dtype=float32), array([2.6872318], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.10508233 0.09655634]\n",
      " [0.88235533 0.8939265 ]\n",
      " [0.9105184  0.9098685 ]\n",
      " [0.09222579 0.08422419]] \n",
      "Correct:\n",
      " [[0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]] \n",
      "Accuracy:  1.0\n",
      "step : 6000 \n",
      "cost : 0.08193207 \n",
      "Weight :\n",
      " [array([[-5.960016 , -5.1586976],\n",
      "       [ 6.2617664,  4.9051614]], dtype=float32), array([[-0.49194542],\n",
      "       [-0.6575601 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 3.2024279, -2.589662 ], dtype=float32), array([2.8395658], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.08375379 0.07792974]\n",
      " [0.9095895  0.9174043 ]\n",
      " [0.92356783 0.9231639 ]\n",
      " [0.07329929 0.06788537]] \n",
      "Correct:\n",
      " [[0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]] \n",
      "Accuracy:  1.0\n",
      "step : 6500 \n",
      "cost : 0.06779916 \n",
      "Weight :\n",
      " [array([[-6.113968 , -5.399152 ],\n",
      "       [ 6.397677 ,  5.1317053]], dtype=float32), array([[-0.49194542],\n",
      "       [-0.6575601 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 3.253159, -2.710654], dtype=float32), array([2.9793923], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.06920576 0.06503314]\n",
      " [0.92719007 0.9327606 ]\n",
      " [0.93374413 0.93347585]\n",
      " [0.06055531 0.05669847]] \n",
      "Correct:\n",
      " [[0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]] \n",
      "Accuracy:  1.0\n",
      "step : 7000 \n",
      "cost : 0.05767323 \n",
      "Weight :\n",
      " [array([[-6.241897 , -5.5916696],\n",
      "       [ 6.5141397,  5.315334 ]], dtype=float32), array([[-0.49194542],\n",
      "       [-0.6575601 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 3.2982068, -2.8086612], dtype=float32), array([3.1065621], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.05875731 0.05564624]\n",
      " [0.9393333  0.94347847]\n",
      " [0.9417734  0.9415857 ]\n",
      " [0.05145225 0.04858688]] \n",
      "Correct:\n",
      " [[0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]] \n",
      "Accuracy:  1.0\n",
      "step : 7500 \n",
      "cost : 0.050090425 \n",
      "Weight :\n",
      " [array([[-6.3507586, -5.750876 ],\n",
      "       [ 6.615515 ,  5.46851  ]], dtype=float32), array([[-0.49194542],\n",
      "       [-0.6575601 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 3.338486, -2.890161], dtype=float32), array([3.2222033], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.05093536 0.04853877]\n",
      " [0.94814754 0.9513391 ]\n",
      " [0.9482106  0.9480736 ]\n",
      " [0.04465207 0.04244992]] \n",
      "Correct:\n",
      " [[0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]] \n",
      "Accuracy:  1.0\n",
      "step : 8000 \n",
      "cost : 0.04421462 \n",
      "Weight :\n",
      " [array([[-6.445149 , -5.8858113],\n",
      "       [ 6.704927 ,  5.5991635]], dtype=float32), array([[-0.49194542],\n",
      "       [-0.6575601 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 3.3747578, -2.959399 ], dtype=float32), array([3.3277125], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.04488242 0.04298601]\n",
      " [0.95480496 0.95733064]\n",
      " [0.95345527 0.95335186]\n",
      " [0.03939289 0.03765321]] \n",
      "Correct:\n",
      " [[0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]] \n",
      "Accuracy:  1.0\n",
      "step : 8500 \n",
      "cost : 0.039536264 \n",
      "Weight :\n",
      " [array([[-6.528234 , -6.00239  ],\n",
      "       [ 6.784672 ,  5.7125945]], dtype=float32), array([[-0.49194542],\n",
      "       [-0.6575601 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 3.4076447, -3.0192568], dtype=float32), array([3.4244254], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.04007107 0.038537  ]\n",
      " [0.9599948  0.9620389 ]\n",
      " [0.95779353 0.95771337]\n",
      " [0.03521165 0.03380603]] \n",
      "Correct:\n",
      " [[0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]] \n",
      "Accuracy:  1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 9000 \n",
      "cost : 0.035728388 \n",
      "Weight :\n",
      " [array([[-6.602276 , -6.1046677],\n",
      "       [ 6.856472 ,  5.812493 ]], dtype=float32), array([[-0.49194542],\n",
      "       [-0.6575601 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 3.4376528, -3.0717554], dtype=float32), array([3.5135114], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.03616178 0.03489748]\n",
      " [0.9641453  0.96583086]\n",
      " [0.9614314  0.9613677 ]\n",
      " [0.03181246 0.03065506]] \n",
      "Correct:\n",
      " [[0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]] \n",
      "Accuracy:  1.0\n",
      "step : 9500 \n",
      "cost : 0.032572065 \n",
      "Weight :\n",
      " [array([[-6.6689377, -6.195531 ],\n",
      "       [ 6.9216466,  5.9015145]], dtype=float32), array([[-0.49194542],\n",
      "       [-0.6575601 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 3.465195 , -3.1183574], dtype=float32), array([3.5959673], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.03292668 0.03186837]\n",
      " [0.96753514 0.9689472 ]\n",
      " [0.96451956 0.964468  ]\n",
      " [0.02899751 0.02802926]] \n",
      "Correct:\n",
      " [[0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]] \n",
      "Accuracy:  1.0\n",
      "step : 10000 \n",
      "cost : 0.029915445 \n",
      "Weight :\n",
      " [array([[-6.729476 , -6.2770987],\n",
      "       [ 6.981229 ,  5.981629 ]], dtype=float32), array([[-0.49194542],\n",
      "       [-0.6575601 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 3.4906073, -3.1601446], dtype=float32), array([3.6726305], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.03020802 0.02931017]\n",
      " [0.97035295 0.97155166]\n",
      " [0.96716964 0.9671272 ]\n",
      " [0.02662995 0.02580893]] \n",
      "Correct:\n",
      " [[0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "## 그래서 2개의 퍼셉트론을 사용해보자\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "x_data = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "W1 = tf.Variable(tf.random_normal([2,2]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([2]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([2,1]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
    "# Hypothesis using sigmoid : tf.div(1., 1. + tf.exp(tf.matmul(X,W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1,W2) + b2)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis > 0.2 else False\n",
    "predicted = tf.cast(hypothesis > 0.2, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 500 == 0:\n",
    "            print(\"step :\", step, \"\\ncost :\", sess.run(cost, feed_dict={X: x_data, Y: y_data}),\n",
    "                  \"\\nWeight :\\n\", sess.run([W1, W2]), \"\\nbias :\\n\", sess.run([b1, b2]))\n",
    "            h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict = {X: x_data, Y: y_data})\n",
    "            print(\"Hypothesis:\\n\", h, \"\\nCorrect:\\n\", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0 \n",
      "cost : 1.1199081 \n",
      "Weight :\n",
      " [array([[-2.113138  ,  0.13639782, -0.98433745, -0.7573919 ,  0.13736415,\n",
      "        -1.9388213 ,  0.8039631 ],\n",
      "       [-1.0594468 ,  2.0178287 , -0.30624238,  0.68230927, -1.726961  ,\n",
      "        -0.5223404 ,  0.6694758 ]], dtype=float32), array([[-0.9251284 ],\n",
      "       [-0.30094695],\n",
      "       [ 1.1453488 ],\n",
      "       [-0.34816572],\n",
      "       [-0.35977682],\n",
      "       [-0.9757601 ],\n",
      "       [-0.3004992 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 0.7939486 ,  0.7278418 ,  0.84283805, -0.26399946,  1.6558917 ,\n",
      "        0.5606624 , -1.0574663 ], dtype=float32), array([-1.2030213], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.08360204]\n",
      " [0.10277629]\n",
      " [0.14111719]\n",
      " [0.14697152]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      "Accuracy:  0.5\n",
      "step : 500 \n",
      "cost : 0.58095217 \n",
      "Weight :\n",
      " [array([[-2.647921  ,  0.29196838, -1.1832287 , -0.76764476,  0.04331074,\n",
      "        -2.2657654 ,  0.839087  ],\n",
      "       [-2.093061  ,  2.0214818 , -0.7035894 ,  0.6654774 , -1.7583352 ,\n",
      "        -1.41439   ,  0.70378643]], dtype=float32), array([[-1.757621  ],\n",
      "       [ 0.1848581 ],\n",
      "       [ 1.8938836 ],\n",
      "       [ 0.06612821],\n",
      "       [ 0.19386922],\n",
      "       [-1.2290695 ],\n",
      "       [-0.450541  ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 0.35605088,  0.7366524 ,  1.3839623 , -0.2917118 ,  1.6237676 ,\n",
      "        0.13028504, -1.1469079 ], dtype=float32), array([-0.6097621], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.35965154]\n",
      " [0.5609648 ]\n",
      " [0.5661248 ]\n",
      " [0.5185862 ]] \n",
      "Correct:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.5\n",
      "step : 1000 \n",
      "cost : 0.40630764 \n",
      "Weight :\n",
      " [array([[-3.3591688 ,  0.37385958, -1.6673887 , -0.83306855, -0.24003562,\n",
      "        -2.7330158 ,  1.0337597 ],\n",
      "       [-3.1201572 ,  2.0245266 , -1.3826406 ,  0.6349549 , -1.8587774 ,\n",
      "        -2.2843914 ,  0.8987032 ]], dtype=float32), array([[-3.2053423 ],\n",
      "       [ 0.02991957],\n",
      "       [ 2.7685068 ],\n",
      "       [ 0.17668699],\n",
      "       [ 0.5946023 ],\n",
      "       [-2.138568  ],\n",
      "       [-1.0939041 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 0.67013395,  0.74966145,  2.2496536 , -0.32387385,  1.5943792 ,\n",
      "        0.17892332, -1.333383  ], dtype=float32), array([-0.73307186], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.23990333]\n",
      " [0.6655104 ]\n",
      " [0.6806526 ]\n",
      " [0.42822903]] \n",
      "Correct:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.5\n",
      "step : 1500 \n",
      "cost : 0.22639295 \n",
      "Weight :\n",
      " [array([[-3.9818225 ,  0.3423273 , -2.320196  , -0.93392825, -0.5644498 ,\n",
      "        -3.1756744 ,  1.3181889 ],\n",
      "       [-3.893619  ,  2.023625  , -2.1062448 ,  0.5975071 , -1.9674169 ,\n",
      "        -2.938741  ,  1.1385477 ]], dtype=float32), array([[-4.4722977 ],\n",
      "       [-0.20601182],\n",
      "       [ 3.9575164 ],\n",
      "       [ 0.28754297],\n",
      "       [ 1.0315684 ],\n",
      "       [-3.00792   ],\n",
      "       [-1.811691  ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 1.1031383 ,  0.74351555,  3.3097768 , -0.3669326 ,  1.6702055 ,\n",
      "        0.49037743, -1.6809334 ], dtype=float32), array([-0.9357088], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.14438823]\n",
      " [0.79418516]\n",
      " [0.81131   ]\n",
      " [0.26661873]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.75\n",
      "step : 2000 \n",
      "cost : 0.12530267 \n",
      "Weight :\n",
      " [array([[-4.3944783 ,  0.27007082, -2.8239458 , -1.0185504 , -0.7732118 ,\n",
      "        -3.503052  ,  1.552635  ],\n",
      "       [-4.3698354 ,  2.0230937 , -2.6368804 ,  0.57680035, -2.00834   ,\n",
      "        -3.366093  ,  1.3360994 ]], dtype=float32), array([[-5.371321  ],\n",
      "       [-0.40881577],\n",
      "       [ 4.93516   ],\n",
      "       [ 0.365704  ],\n",
      "       [ 1.3141371 ],\n",
      "       [-3.672037  ],\n",
      "       [-2.3678975 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 1.3972136 ,  0.7311596 ,  4.10117   , -0.40264615,  1.8221884 ,\n",
      "        0.7717685 , -2.0387638 ], dtype=float32), array([-1.1208999], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.08668536]\n",
      " [0.8804672 ]\n",
      " [0.8910963 ]\n",
      " [0.15458736]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n",
      "step : 2500 \n",
      "cost : 0.07797799 \n",
      "Weight :\n",
      " [array([[-4.658801  ,  0.19939832, -3.1547868 , -1.0776509 , -0.91039515,\n",
      "        -3.731305  ,  1.7244004 ],\n",
      "       [-4.663024  ,  2.0245864 , -2.9841938 ,  0.56863225, -2.0223076 ,\n",
      "        -3.6445422 ,  1.4932506 ]], dtype=float32), array([[-5.9829698 ],\n",
      "       [-0.5578192 ],\n",
      "       [ 5.622025  ],\n",
      "       [ 0.41607106],\n",
      "       [ 1.4904755 ],\n",
      "       [-4.147687  ],\n",
      "       [-2.7628398 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 1.5816675 ,  0.7209651 ,  4.619258  , -0.42821553,  1.9604021 ,\n",
      "        0.96607214, -2.3225002 ], dtype=float32), array([-1.2582593], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.05682611]\n",
      " [0.92405844]\n",
      " [0.9308162 ]\n",
      " [0.09763294]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n",
      "step : 3000 \n",
      "cost : 0.053905442 \n",
      "Weight :\n",
      " [array([[-4.840046  ,  0.13846065, -3.3813152 , -1.1198152 , -1.0097803 ,\n",
      "        -3.8964765 ,  1.8537548 ],\n",
      "       [-4.8599653 ,  2.0274181 , -3.2211332 ,  0.5663814 , -2.0289679 ,\n",
      "        -3.8387792 ,  1.6184703 ]], dtype=float32), array([[-6.4214654 ],\n",
      "       [-0.66822696],\n",
      "       [ 6.115241  ],\n",
      "       [ 0.4510873 ],\n",
      "       [ 1.6116991 ],\n",
      "       [-4.499857  ],\n",
      "       [-3.053918  ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 1.705688  ,  0.71343124,  4.9736433 , -0.44686568,  2.0716004 ,\n",
      "        1.1016346 , -2.5414755 ], dtype=float32), array([-1.3592355], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.04062617]\n",
      " [0.9469532 ]\n",
      " [0.9516861 ]\n",
      " [0.06771958]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n",
      "step : 3500 \n",
      "cost : 0.04015543 \n",
      "Weight :\n",
      " [array([[-4.973313  ,  0.08657175, -3.5469046 , -1.1515737 , -1.0867653 ,\n",
      "        -4.022288  ,  1.9556437 ],\n",
      "       [-5.0031314 ,  2.0309393 , -3.3934505 ,  0.56685114, -2.0335665 ,\n",
      "        -3.9835517 ,  1.720235  ]], dtype=float32), array([[-6.754862 ],\n",
      "       [-0.7535539],\n",
      "       [ 6.4880986],\n",
      "       [ 0.4773951],\n",
      "       [ 1.7022185],\n",
      "       [-4.7733855],\n",
      "       [-3.2797163]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 1.7955421,  0.7078557,  5.23234  , -0.4611383,  2.161201 ,\n",
      "        1.2015821, -2.7150161], dtype=float32), array([-1.4362816], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.03098056]\n",
      " [0.960237  ]\n",
      " [0.96381736]\n",
      " [0.05040765]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n",
      "step : 4000 \n",
      "cost : 0.031524144 \n",
      "Weight :\n",
      " [array([[-5.07671   ,  0.04191753, -3.6745946 , -1.1766486 , -1.1491555 ,\n",
      "        -4.1223254 ,  2.0389721 ],\n",
      "       [-5.1134515 ,  2.0347857 , -3.525613  ,  0.5686305 , -2.0375345 ,\n",
      "        -4.0970826 ,  1.8048863 ]], dtype=float32), array([[-7.0204    ],\n",
      "       [-0.8221375 ],\n",
      "       [ 6.782912  ],\n",
      "       [ 0.49831542],\n",
      "       [ 1.773833  ],\n",
      "       [-4.994567  ],\n",
      "       [-3.462338  ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 1.8644855 ,  0.70366055,  5.4315224 , -0.4725364 ,  2.2350295 ,\n",
      "        1.2790239 , -2.8568559 ], dtype=float32), array([-1.4973642], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.02474865]\n",
      " [0.96865016]\n",
      " [0.97151124]\n",
      " [0.03948173]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n",
      "step : 4500 \n",
      "cost : 0.02570441 \n",
      "Weight :\n",
      " [array([[-5.1601973e+00,  2.9381353e-03, -3.7771242e+00, -1.1971825e+00,\n",
      "        -1.2013696e+00, -4.2045717e+00,  2.1091249e+00],\n",
      "       [-5.2021480e+00,  2.0387197e+00, -3.6311831e+00,  5.7106107e-01,\n",
      "        -2.0412858e+00, -4.1895452e+00,  1.8768065e+00]], dtype=float32), array([[-7.2394195 ],\n",
      "       [-0.8790228 ],\n",
      "       [ 7.024311  ],\n",
      "       [ 0.51563853],\n",
      "       [ 1.8328316 ],\n",
      "       [-5.179087  ],\n",
      "       [-3.614814  ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 1.9196756 ,  0.70045817,  5.591206  , -0.48194778,  2.2972727 ,\n",
      "        1.341398  , -2.9758873 ], dtype=float32), array([-1.547351], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.02045596]\n",
      " [0.97435504]\n",
      " [0.9767325 ]\n",
      " [0.03210104]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n",
      "step : 5000 \n",
      "cost : 0.021561066 \n",
      "Weight :\n",
      " [array([[-5.2296567 , -0.03154615, -3.862019  , -1.2144744 , -1.2461258 ,\n",
      "        -4.2739635 ,  2.1695135 ],\n",
      "       [-5.275738  ,  2.0427089 , -3.7181702 ,  0.5738137 , -2.0449696 ,\n",
      "        -4.267027  ,  1.9390119 ]], dtype=float32), array([[-7.4249244],\n",
      "       [-0.92738  ],\n",
      "       [ 7.2273855],\n",
      "       [ 0.5304121],\n",
      "       [ 1.8828876],\n",
      "       [-5.336775 ],\n",
      "       [-3.7452502]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 1.9652815 ,  0.697991  ,  5.7232256 , -0.48992229,  2.3507974 ,\n",
      "        1.3931594 , -3.0779347 ], dtype=float32), array([-1.589304], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.01734892]\n",
      " [0.97843266]\n",
      " [0.980466  ]\n",
      " [0.0268456 ]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n",
      "step : 5500 \n",
      "cost : 0.0184843 \n",
      "Weight :\n",
      " [array([[-5.2887993 , -0.06241081, -3.9340036 , -1.2293575 , -1.2852045 ,\n",
      "        -4.3337097 ,  2.2224162 ],\n",
      "       [-5.3382826 ,  2.0466428 , -3.791597  ,  0.57671463, -2.0485458 ,\n",
      "        -4.3334002 ,  1.9936235 ]], dtype=float32), array([[-7.585309 ],\n",
      "       [-0.969297 ],\n",
      "       [ 7.401861 ],\n",
      "       [ 0.5432917],\n",
      "       [ 1.9262997],\n",
      "       [-5.4740996],\n",
      "       [-3.85896  ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 2.0038977 ,  0.69608444,  5.8350034 , -0.49681816,  2.3975863 ,\n",
      "        1.4371239 , -3.1669395 ], dtype=float32), array([-1.6252369], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.01501045]\n",
      " [0.9814698 ]\n",
      " [0.9832466 ]\n",
      " [0.02294621]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n",
      "step : 6000 \n",
      "cost : 0.016121864 \n",
      "Weight :\n",
      " [array([[-5.3400855 , -0.09031206, -3.9961965 , -1.2423899 , -1.3198276 ,\n",
      "        -4.385993  ,  2.2694125 ],\n",
      "       [-5.3924527 ,  2.050546  , -3.8547726 ,  0.57966506, -2.0521164 ,\n",
      "        -4.39126   ,  2.0421643 ]], dtype=float32), array([[-7.726254  ],\n",
      "       [-1.0061997 ],\n",
      "       [ 7.554313  ],\n",
      "       [ 0.55471456],\n",
      "       [ 1.9645982 ],\n",
      "       [-5.5955076 ],\n",
      "       [-3.9595904 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 2.037229  ,  0.69461447,  5.9314494 , -0.5028781 ,  2.43905   ,\n",
      "        1.4751621 , -3.2456703 ], dtype=float32), array([-1.6565181], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.01319504]\n",
      " [0.9838071 ]\n",
      " [0.98538625]\n",
      " [0.01995578]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 6500 \n",
      "cost : 0.014258415 \n",
      "Weight :\n",
      " [array([[-5.38522   , -0.11574854, -4.0507517 , -1.2539616 , -1.3508695 ,\n",
      "        -4.432354  ,  2.3116434 ],\n",
      "       [-5.440084  ,  2.0543606 , -3.9099777 ,  0.5826155 , -2.0555735 ,\n",
      "        -4.4424124 ,  2.0857644 ]], dtype=float32), array([[-7.851757 ],\n",
      "       [-1.0391065],\n",
      "       [ 7.6893516],\n",
      "       [ 0.5649819],\n",
      "       [ 1.9988439],\n",
      "       [-5.7041693],\n",
      "       [-4.04974  ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 2.0664482 ,  0.69349176,  6.015938  , -0.50827426,  2.4762132 ,\n",
      "        1.5085682 , -3.3161206 ], dtype=float32), array([-1.6841218], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.01174933]\n",
      " [0.9856545 ]\n",
      " [0.9870764 ]\n",
      " [0.01760077]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n",
      "step : 7000 \n",
      "cost : 0.012755479 \n",
      "Weight :\n",
      " [array([[-5.425422  , -0.13910693, -4.099206  , -1.2643551 , -1.3789731 ,\n",
      "        -4.473919  ,  2.3499498 ],\n",
      "       [-5.4824924 ,  2.0580728 , -3.9588325 ,  0.585533  , -2.0590012 ,\n",
      "        -4.488159  ,  2.125271  ]], dtype=float32), array([[-7.964726  ],\n",
      "       [-1.0687578 ],\n",
      "       [ 7.81032   ],\n",
      "       [ 0.57431287],\n",
      "       [ 2.029806  ],\n",
      "       [-5.802411  ],\n",
      "       [-4.131316  ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 2.0923858 ,  0.6926499 ,  6.090889  , -0.51313144,  2.5098424 ,\n",
      "        1.5382687 , -3.3797767 ], dtype=float32), array([-1.7087505], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.01057383]\n",
      " [0.98714674]\n",
      " [0.9884412 ]\n",
      " [0.01570445]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n",
      "step : 7500 \n",
      "cost : 0.011520704 \n",
      "Weight :\n",
      " [array([[-5.4615912 , -0.16069111, -4.14269   , -1.2737796 , -1.4046282 ,\n",
      "        -4.511524  ,  2.3849761 ],\n",
      "       [-5.5206347 ,  2.0617015 , -4.002533  ,  0.5884009 , -2.062339  ,\n",
      "        -4.52947   ,  2.1613433 ]], dtype=float32), array([[-8.067342 ],\n",
      "       [-1.0957148],\n",
      "       [ 7.9197097],\n",
      "       [ 0.5828683],\n",
      "       [ 2.058052 ],\n",
      "       [-5.89199  ],\n",
      "       [-4.20576  ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 2.115654  ,  0.692038  ,  6.158076  , -0.51754415,  2.5405207 ,\n",
      "        1.564947  , -3.4377646 ], dtype=float32), array([-1.7309358], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.00960112]\n",
      " [0.9883746 ]\n",
      " [0.9895633 ]\n",
      " [0.01414919]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n",
      "step : 8000 \n",
      "cost : 0.010490088 \n",
      "Weight :\n",
      " [array([[-5.4944143 , -0.18074438, -4.182059  , -1.2823962 , -1.4282113 ,\n",
      "        -4.545816  ,  2.4172192 ],\n",
      "       [-5.5552444 ,  2.0652463 , -4.041975  ,  0.59120786, -2.0656056 ,\n",
      "        -4.567082  ,  2.1944942 ]], dtype=float32), array([[-8.161263 ],\n",
      "       [-1.1204047],\n",
      "       [ 8.019421 ],\n",
      "       [ 0.5907726],\n",
      "       [ 2.0840206],\n",
      "       [-5.9742646],\n",
      "       [-4.2741814]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 2.1367142 ,  0.69161665,  6.2188344 , -0.5215831 ,  2.568704  ,\n",
      "        1.5891192 , -3.4909632 ], dtype=float32), array([-1.7510797], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.00878409]\n",
      " [0.98940074]\n",
      " [0.99050045]\n",
      " [0.01285338]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n",
      "step : 8500 \n",
      "cost : 0.009618282 \n",
      "Weight :\n",
      " [array([[-5.524416  , -0.19946398, -4.217966  , -1.2903264 , -1.4500216 ,\n",
      "        -4.577296  ,  2.4470756 ],\n",
      "       [-5.586876  ,  2.0687034 , -4.077846  ,  0.5939496 , -2.0688243 ,\n",
      "        -4.6015654 ,  2.2251358 ]], dtype=float32), array([[-8.247807  ],\n",
      "       [-1.1431664 ],\n",
      "       [ 8.110942  ],\n",
      "       [ 0.59812146],\n",
      "       [ 2.1080487 ],\n",
      "       [-6.0502987 ],\n",
      "       [-4.3374553 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 2.155919  ,  0.69135493,  6.2741995 , -0.5253051 ,  2.594751  ,\n",
      "        1.6111838 , -3.5400634 ], dtype=float32), array([-1.7694974], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.00808898]\n",
      " [0.99026966]\n",
      " [0.99129355]\n",
      " [0.01175904]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n",
      "step : 9000 \n",
      "cost : 0.008872241 \n",
      "Weight :\n",
      " [array([[-5.5520096 , -0.21701205, -4.2509346 , -1.2976704 , -1.4702979 ,\n",
      "        -4.6063657 ,  2.4748614 ],\n",
      "       [-5.615977  ,  2.0720491 , -4.110691  ,  0.5966249 , -2.071925  ,\n",
      "        -4.6333737 ,  2.2535987 ]], dtype=float32), array([[-8.327991 ],\n",
      "       [-1.1642689],\n",
      "       [ 8.195446 ],\n",
      "       [ 0.6049911],\n",
      "       [ 2.130405 ],\n",
      "       [-6.1209464],\n",
      "       [-4.396282 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 2.1735487 ,  0.69122916,  6.3249817 , -0.52875495,  2.6189487 ,\n",
      "        1.6314543 , -3.5856202 ], dtype=float32), array([-1.7864392], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.00749108]\n",
      " [0.99101424]\n",
      " [0.9919723 ]\n",
      " [0.01082423]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n",
      "step : 9500 \n",
      "cost : 0.0082272105 \n",
      "Weight :\n",
      " [array([[-5.5775337 , -0.23352326, -4.281373  , -1.3045063 , -1.4892339 ,\n",
      "        -4.6333437 ,  2.500836  ],\n",
      "       [-5.642896  ,  2.0753229 , -4.1409397 ,  0.5992318 , -2.074982  ,\n",
      "        -4.662869  ,  2.2801552 ]], dtype=float32), array([[-8.402665  ],\n",
      "       [-1.1839273 ],\n",
      "       [ 8.2738695 ],\n",
      "       [ 0.61144346],\n",
      "       [ 2.1513083 ],\n",
      "       [-6.1868997 ],\n",
      "       [-4.451229  ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 2.189825  ,  0.6912174 ,  6.3718247 , -0.53196776,  2.641536  ,\n",
      "        1.650181  , -3.6280878 ], dtype=float32), array([-1.8021052], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.00697163]\n",
      " [0.9916585 ]\n",
      " [0.9925594 ]\n",
      " [0.0100174 ]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n",
      "step : 10000 \n",
      "cost : 0.007664486 \n",
      "Weight :\n",
      " [array([[-5.601255  , -0.24911036, -4.3096185 , -1.3108987 , -1.506991  ,\n",
      "        -4.658496  ,  2.5252142 ],\n",
      "       [-5.667921  ,  2.078534  , -4.168942  ,  0.60177165, -2.0779622 ,\n",
      "        -4.690348  ,  2.3050308 ]], dtype=float32), array([[-8.4725065],\n",
      "       [-1.2023221],\n",
      "       [ 8.346994 ],\n",
      "       [ 0.6175286],\n",
      "       [ 2.1709352],\n",
      "       [-6.248727 ],\n",
      "       [-4.502766 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 2.2049255,  0.6913042,  6.4152546, -0.5349741,  2.6627042,\n",
      "        1.6675675, -3.6678374], dtype=float32), array([-1.8166612], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.00651658]\n",
      " [0.9922211 ]\n",
      " [0.99307156]\n",
      " [0.00931451]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "## 퍼셉트론의 개수를 7개까지 증가시켜보자\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "x_data = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "W1 = tf.Variable(tf.random_normal([2,7]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([7]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([7,1]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
    "# Hypothesis using sigmoid : tf.div(1., 1. + tf.exp(tf.matmul(X,W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1,W2) + b2)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis > 0.2 else False\n",
    "predicted = tf.cast(hypothesis > 0.2, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 500 == 0:\n",
    "            print(\"step :\", step, \"\\ncost :\", sess.run(cost, feed_dict={X: x_data, Y: y_data}),\n",
    "                  \"\\nWeight :\\n\", sess.run([W1, W2]), \"\\nbias :\\n\", sess.run([b1, b2]))\n",
    "            h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict = {X: x_data, Y: y_data})\n",
    "            print(\"Hypothesis:\\n\", h, \"\\nCorrect:\\n\", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0 \n",
      "cost : 0.692396 \n",
      "Weight :\n",
      " [array([[ 0.044613  , -0.18088494, -0.89101404],\n",
      "       [-2.0226157 ,  0.8154059 , -1.4444437 ]], dtype=float32), array([[-0.85337716, -2.1313858 , -0.7931945 ],\n",
      "       [ 0.7735773 ,  0.67552716, -0.86295116],\n",
      "       [ 0.37045872, -1.1534873 ,  0.7671437 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 0.5504816 , -0.7536897 , -0.71946543], dtype=float32), array([-0.53694993, -0.35990393, -0.8781324 ], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.49054167]\n",
      " [0.5549633 ]\n",
      " [0.50552833]\n",
      " [0.5614021 ]] \n",
      "Correct:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.5\n",
      "step : 500 \n",
      "cost : 0.6847033 \n",
      "Weight :\n",
      " [array([[ 0.38660133, -0.3390861 , -1.0882457 ],\n",
      "       [-1.9355185 ,  0.8009414 , -1.6210877 ]], dtype=float32), array([[-0.8829346 , -2.083798  , -0.7974631 ],\n",
      "       [ 0.79662514,  0.7135403 , -0.8463065 ],\n",
      "       [ 0.53354096, -1.1900613 ,  0.8433643 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 0.57840574, -0.75515157, -0.7102253 ], dtype=float32), array([-0.45420894, -0.38713175, -0.8298085 ], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.4623984 ]\n",
      " [0.5236086 ]\n",
      " [0.48286262]\n",
      " [0.52438265]] \n",
      "Correct:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.5\n",
      "step : 1000 \n",
      "cost : 0.6715244 \n",
      "Weight :\n",
      " [array([[ 0.88216144, -0.5259325 , -1.3347316 ],\n",
      "       [-1.8331225 ,  0.8215711 , -1.8523612 ]], dtype=float32), array([[-0.8439023 , -2.1907797 , -0.7737733 ],\n",
      "       [ 0.7963419 ,  0.86665374, -0.8381204 ],\n",
      "       [ 0.77995527, -1.2699454 ,  0.9651008 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 0.66542107, -0.7365975 , -0.6133438 ], dtype=float32), array([-0.3473102 , -0.34633198, -0.768324  ], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.44928348]\n",
      " [0.53872365]\n",
      " [0.48188183]\n",
      " [0.5233409 ]] \n",
      "Correct:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.5\n",
      "step : 1500 \n",
      "cost : 0.63072824 \n",
      "Weight :\n",
      " [array([[ 1.6320715 , -0.82655376, -1.6948963 ],\n",
      "       [-1.8196193 ,  0.94505435, -2.1734064 ]], dtype=float32), array([[-0.6806462 , -2.5914657 , -0.6858377 ],\n",
      "       [ 0.8064725 ,  1.2373565 , -0.82404023],\n",
      "       [ 1.1368617 , -1.4122497 ,  1.1646876 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 0.76259816, -0.6672839 , -0.38745987], dtype=float32), array([-0.22420721, -0.12303942, -0.69106436], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.40711045]\n",
      " [0.60884225]\n",
      " [0.46042302]\n",
      " [0.51730007]] \n",
      "Correct:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.5\n",
      "step : 2000 \n",
      "cost : 0.540014 \n",
      "Weight :\n",
      " [array([[ 2.5610049, -1.3132957, -2.2722163],\n",
      "       [-2.0635395,  1.1972735, -2.5623434]], dtype=float32), array([[-0.5665373 , -3.39389   , -0.59614784],\n",
      "       [ 0.93541443,  1.8125813 , -0.7325008 ],\n",
      "       [ 1.5872669 , -1.653114  ,  1.479251  ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 0.8304193 , -0.59635097, -0.03069468], dtype=float32), array([-0.19502047,  0.15557387, -0.6151064 ], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.3131494 ]\n",
      " [0.76505303]\n",
      " [0.42293566]\n",
      " [0.48111466]] \n",
      "Correct:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.5\n",
      "step : 2500 \n",
      "cost : 0.44905734 \n",
      "Weight :\n",
      " [array([[ 3.2086167, -1.7021887, -2.9837346],\n",
      "       [-2.3615017,  1.5325673, -2.895616 ]], dtype=float32), array([[-0.70223737, -4.266854  , -0.6361535 ],\n",
      "       [ 1.3280897 ,  2.274669  , -0.49890083],\n",
      "       [ 2.0512998 , -1.8695831 ,  1.8736778 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 0.80333334, -0.49043924,  0.29670066], dtype=float32), array([-0.276071  ,  0.18726684, -0.5897401 ], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.20018068]\n",
      " [0.8708664 ]\n",
      " [0.43991843]\n",
      " [0.45850828]] \n",
      "Correct:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.5\n",
      "step : 3000 \n",
      "cost : 0.32346857 \n",
      "Weight :\n",
      " [array([[ 3.6015625, -1.6585664, -3.5950732],\n",
      "       [-2.5799732,  2.4237106, -3.1501312]], dtype=float32), array([[-0.98067915, -5.1313267 , -0.78451407],\n",
      "       [ 2.427461  ,  2.448739  , -0.11952896],\n",
      "       [ 2.3675978 , -1.9776578 ,  2.215645  ]], dtype=float32)] \n",
      "bias :\n",
      " [array([ 0.68409353, -0.06562234,  0.43721792], dtype=float32), array([-0.32378912, -0.03435586, -0.6986024 ], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.1199494 ]\n",
      " [0.8946766 ]\n",
      " [0.54667026]\n",
      " [0.3629429 ]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.75\n",
      "step : 3500 \n",
      "cost : 0.16434257 \n",
      "Weight :\n",
      " [array([[ 3.8780015, -1.8912729, -3.9361522],\n",
      "       [-2.801812 ,  3.4537556, -3.3513055]], dtype=float32), array([[-1.5080447 , -5.8601584 , -0.8138183 ],\n",
      "       [ 3.914369  ,  2.5335853 ,  0.23203336],\n",
      "       [ 2.5588076 , -2.0170496 ,  2.4582524 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([0.47982055, 0.07464932, 0.5302487 ], dtype=float32), array([-0.57042944, -0.1078563 , -0.8471662 ], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.06852099]\n",
      " [0.9091246 ]\n",
      " [0.76468813]\n",
      " [0.19974774]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n",
      "step : 4000 \n",
      "cost : 0.09523881 \n",
      "Weight :\n",
      " [array([[ 4.0810647, -2.1388428, -4.09313  ],\n",
      "       [-2.912486 ,  3.9621007, -3.491368 ]], dtype=float32), array([[-1.8186257 , -6.320415  , -0.74357957],\n",
      "       [ 4.794467  ,  2.666387  ,  0.42030308],\n",
      "       [ 2.6810565 , -2.0394962 ,  2.604601  ]], dtype=float32)] \n",
      "bias :\n",
      " [array([0.37966442, 0.10865593, 0.6078149 ], dtype=float32), array([-0.68773943, -0.05737884, -0.94937515], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.04149365]\n",
      " [0.9334642 ]\n",
      " [0.86771536]\n",
      " [0.11999875]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n",
      "step : 4500 \n",
      "cost : 0.0647686 \n",
      "Weight :\n",
      " [array([[ 4.230403 , -2.2973208, -4.1827865],\n",
      "       [-2.965755 ,  4.238891 , -3.5873182]], dtype=float32), array([[-2.000246 , -6.6269956, -0.6619358],\n",
      "       [ 5.3192973,  2.7718472,  0.5393047],\n",
      "       [ 2.7574208, -2.0577626,  2.6998105]], dtype=float32)] \n",
      "bias :\n",
      " [array([0.34488967, 0.13831158, 0.65974176], dtype=float32), array([-0.7516631 , -0.00532059, -1.0241469 ], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.02859041]\n",
      " [0.9503283 ]\n",
      " [0.9116734 ]\n",
      " [0.08299839]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n",
      "step : 5000 \n",
      "cost : 0.04845024 \n",
      "Weight :\n",
      " [array([[ 4.3405986, -2.4061868, -4.243469 ],\n",
      "       [-2.9997857,  4.4176483, -3.6576176]], dtype=float32), array([[-2.1217403 , -6.8482757 , -0.5861455 ],\n",
      "       [ 5.6733384 ,  2.8520763 ,  0.62956953],\n",
      "       [ 2.8094993 , -2.0733144 ,  2.7680297 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([0.33240336, 0.16285905, 0.69580907], dtype=float32), array([-0.7954675 ,  0.03700306, -1.0823889 ], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.02145478]\n",
      " [0.96108663]\n",
      " [0.93450725]\n",
      " [0.06263825]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n",
      "step : 5500 \n",
      "cost : 0.038457938 \n",
      "Weight :\n",
      " [array([[ 4.42529  , -2.4866924, -4.2887497],\n",
      "       [-3.0252292,  4.5461755, -3.7122495]], dtype=float32), array([[-2.2107375 , -7.0181794 , -0.51804507],\n",
      "       [ 5.933642  ,  2.9149122 ,  0.70463496],\n",
      "       [ 2.8478072 , -2.0867338 ,  2.8202076 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([0.32883143, 0.18321429, 0.7222622 ], dtype=float32), array([-0.8289094 ,  0.07112557, -1.1301659 ], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.01702842]\n",
      " [0.9682683 ]\n",
      " [0.94822836]\n",
      " [0.04995948]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n",
      "step : 6000 \n",
      "cost : 0.03176488 \n",
      "Weight :\n",
      " [array([[ 4.492994 , -2.5495095, -4.3246346],\n",
      "       [-3.045803 ,  4.645107 , -3.7565773]], dtype=float32), array([[-2.2799175 , -7.1545343 , -0.45685136],\n",
      "       [ 6.136318  ,  2.9657934 ,  0.7701205 ],\n",
      "       [ 2.8775775 , -2.0984597 ,  2.861947  ]], dtype=float32)] \n",
      "bias :\n",
      " [array([0.32938603, 0.2004604 , 0.74253124], dtype=float32), array([-0.8559636 ,  0.09922747, -1.1708871 ], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.01405033]\n",
      " [0.9733269 ]\n",
      " [0.9573138 ]\n",
      " [0.04137009]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n",
      "step : 6500 \n",
      "cost : 0.026990136 \n",
      "Weight :\n",
      " [array([[ 4.548872 , -2.600473 , -4.3542485],\n",
      "       [-3.0631762,  4.724846 , -3.7937014]], dtype=float32), array([[-2.335946  , -7.267553  , -0.40150625],\n",
      "       [ 6.3005686 ,  3.0081668 ,  0.82883555],\n",
      "       [ 2.901655  , -2.1088293 ,  2.896433  ]], dtype=float32)] \n",
      "bias :\n",
      " [array([0.33203623, 0.21539211, 0.7585928 ], dtype=float32), array([-0.87865704,  0.12291205, -1.2066061 ], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.01192501]\n",
      " [0.9770552 ]\n",
      " [0.96374846]\n",
      " [0.03519264]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n",
      "step : 7000 \n",
      "cost : 0.023422284 \n",
      "Weight :\n",
      " [array([[ 4.5961685, -2.6430418, -4.3794017],\n",
      "       [-3.0782504,  4.791274 , -3.8255394]], dtype=float32), array([[-2.3826947 , -7.3635297 , -0.35109153],\n",
      "       [ 6.4376326 ,  3.0442593 ,  0.8823819 ],\n",
      "       [ 2.9217162 , -2.1181014 ,  2.925629  ]], dtype=float32)] \n",
      "bias :\n",
      " [array([0.33580634, 0.22855806, 0.77166146], dtype=float32), array([-0.89817333,  0.14327925, -1.2386382 ], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.01033956]\n",
      " [0.9799051 ]\n",
      " [0.9685348 ]\n",
      " [0.03054902]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n",
      "step : 7500 \n",
      "cost : 0.020660134 \n",
      "Weight :\n",
      " [array([[ 4.6370125, -2.679409 , -4.401231 ],\n",
      "       [-3.0915723,  4.847994 , -3.8533492]], dtype=float32), array([[-2.4225874 , -7.4465823 , -0.30486897],\n",
      "       [ 6.5545845 ,  3.0755646 ,  0.9317697 ],\n",
      "       [ 2.9388201 , -2.126471  ,  2.9508188 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([0.34018445, 0.24034007, 0.78252333], dtype=float32), array([-0.91526765,  0.16108777, -1.2678682 ], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.00911546]\n",
      " [0.982149  ]\n",
      " [0.97222954]\n",
      " [0.02693823]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 8000 \n",
      "cost : 0.018461414 \n",
      "Weight :\n",
      " [array([[ 4.6728606, -2.7110353, -4.420491 ],\n",
      "       [-3.1035068,  4.897349 , -3.8779974]], dtype=float32), array([[-2.457233  , -7.5195374 , -0.26224995],\n",
      "       [ 6.6561356 ,  3.103119  ,  0.9776851 ],\n",
      "       [ 2.95367   , -2.1340911 ,  2.9728894 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([0.34488362, 0.2510096 , 0.79171115], dtype=float32), array([-0.9304525,  0.1768736, -1.2949125], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.00814405]\n",
      " [0.9839584 ]\n",
      " [0.9751655 ]\n",
      " [0.0240545 ]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n",
      "step : 8500 \n",
      "cost : 0.016671538 \n",
      "Weight :\n",
      " [array([[ 4.7047396, -2.7389386, -4.4377146],\n",
      "       [-3.1143165,  4.9409504, -3.9001014]], dtype=float32), array([[-2.4877458 , -7.584414  , -0.22276202],\n",
      "       [ 6.745559  ,  3.127665  ,  1.0206227 ],\n",
      "       [ 2.9667523 , -2.1410794 ,  2.9924726 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([0.34973505, 0.26076606, 0.7995984 ], dtype=float32), array([-0.94409245,  0.1910265 , -1.320216  ], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.00735575]\n",
      " [0.9854465 ]\n",
      " [0.97755337]\n",
      " [0.02170143]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n",
      "step : 9000 \n",
      "cost : 0.015187361 \n",
      "Weight :\n",
      " [array([[ 4.733397 , -2.76385  , -4.453273 ],\n",
      "       [-3.1241872,  4.9799376, -3.9201195]], dtype=float32), array([[-2.5149302 , -7.642693  , -0.18601987],\n",
      "       [ 6.825218  ,  3.149753  ,  1.0609578 ],\n",
      "       [ 2.9784157 , -2.147527  ,  3.0100322 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([0.35463566, 0.26975852, 0.80645543], dtype=float32), array([-0.9564562 ,  0.20383579, -1.344104  ], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.00670391]\n",
      " [0.98669076]\n",
      " [0.9795325 ]\n",
      " [0.01974693]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n",
      "step : 9500 \n",
      "cost : 0.013937529 \n",
      "Weight :\n",
      " [array([[ 4.7593994, -2.7863154, -4.467459 ],\n",
      "       [-3.1332684,  5.0151553, -3.9383965]], dtype=float32), array([[-2.5393803 , -7.695496  , -0.15170525],\n",
      "       [ 6.896869  ,  3.1697974 ,  1.0989842 ],\n",
      "       [ 2.9889183 , -2.1535132 ,  3.0259202 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([0.35952404, 0.27810177, 0.8124822 ], dtype=float32), array([-0.9677479 ,  0.21552183, -1.3668212 ], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.00615641]\n",
      " [0.987746  ]\n",
      " [0.98119926]\n",
      " [0.01809946]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n",
      "step : 10000 \n",
      "cost : 0.012871255 \n",
      "Weight :\n",
      " [array([[ 4.7831793, -2.8067448, -4.480496 ],\n",
      "       [-3.1416738,  5.047237 , -3.9552014]], dtype=float32), array([[-2.56155   , -7.7436914 , -0.11955021],\n",
      "       [ 6.9618454 ,  3.188118  ,  1.1349424 ],\n",
      "       [ 2.9984577 , -2.1590896 ,  3.0404074 ]], dtype=float32)] \n",
      "bias :\n",
      " [array([0.36435798, 0.2858863 , 0.8178306 ], dtype=float32), array([-0.97812635,  0.22625592, -1.3885525 ], dtype=float32)]\n",
      "Hypothesis:\n",
      " [[0.00569046]\n",
      " [0.9886515 ]\n",
      " [0.9826217 ]\n",
      " [0.016693  ]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "## layer 2개 놓고 문제풀기\n",
    "\n",
    "x_data = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "W1 = tf.Variable(tf.random_normal([2,3]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([3]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([3,3]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([3]), name='bias2')\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1,W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([3,1]), name='weight3')\n",
    "b3 = tf.Variable(tf.random_normal([1]), name='bias3')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis > 0.2 else False\n",
    "predicted = tf.cast(hypothesis > 0.2, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 500 == 0:\n",
    "            print(\"step :\", step, \"\\ncost :\", sess.run(cost, feed_dict={X: x_data, Y: y_data}),\n",
    "                  \"\\nWeight :\\n\", sess.run([W1, W2]), \"\\nbias :\\n\", sess.run([b1, b2]))\n",
    "            h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict = {X: x_data, Y: y_data})\n",
    "            print(\"Hypothesis:\\n\", h, \"\\nCorrect:\\n\", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
